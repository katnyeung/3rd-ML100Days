{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 作業目的: 熟練自定義collate_fn與sampler進行資料讀取\n",
    "\n",
    "本此作業主要會使用[IMDB](http://ai.stanford.edu/~amaas/data/sentiment/)資料集利用Pytorch的Dataset與DataLoader進行\n",
    "客製化資料讀取。\n",
    "下載後的資料有分成train與test，因為這份作業目的在讀取資料，所以我們取用train部分來進行練習。\n",
    "(請同學先行至IMDB下載資料)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 載入套件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/katnyeung/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/katnyeung/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import torch and other required modules\n",
    "import glob\n",
    "import torch\n",
    "import re\n",
    "import nltk\n",
    "import os\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from sklearn.datasets import load_svmlight_file\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords') #下載stopwords\n",
    "nltk.download('punkt') #下載word_tokenize需要的corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 探索資料與資料前處理\n",
    "這份作業我們使用test資料中的pos與neg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab length before removing stopwords: 89527\n",
      "vocab length after removing stopwords: 89356\n"
     ]
    }
   ],
   "source": [
    "# 讀取字典，這份字典為review內所有出現的字詞\n",
    "with open(os.path.join('aclImdb', 'imdb.vocab'), encoding='utf-8') as f:\n",
    "    vocab = [line.strip() for line in f.readlines()]\n",
    "\n",
    "# 以nltk stopwords移除贅字，過多的贅字無法提供有用的訊息，也可能影響模型的訓練\n",
    "print(f\"vocab length before removing stopwords: {len(vocab)}\")\n",
    "en_stopwords = set(stopwords.words('english'))\n",
    "vocab = [word for word in vocab if word not in en_stopwords]\n",
    "print(f\"vocab length after removing stopwords: {len(vocab)}\")\n",
    "\n",
    "# 將字典轉換成dictionary\n",
    "vocab_dic = {word: idx for idx, word in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('aclImdb/train/pos/190_10.txt', 1), ('aclImdb/train/pos/3395_9.txt', 1)]\n",
      "Total reviews: 25000\n"
     ]
    }
   ],
   "source": [
    "# 將資料打包成(x, y)配對，其中x為review的檔案路徑，y為正評(1)或負評(0)\n",
    "# 這裡將x以檔案路徑代表的原因是讓同學練習不一次將資料全讀取進來，若電腦記憶體夠大(所有資料檔案沒有很大)\n",
    "# 可以將資料全一次讀取，可以減少在訓練時I/O時間，增加訓練速度\n",
    "\n",
    "review_pairs = []\n",
    "for folder, label in [('pos', 1), ('neg', 0)]:\n",
    "    filepaths = glob.glob(os.path.join('aclImdb', 'train', folder, '*'))\n",
    "    for filepath in filepaths:\n",
    "        review_pairs.append((filepath, label))\n",
    "\n",
    "print(review_pairs[:2])\n",
    "print(f\"Total reviews: {len(review_pairs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 建立Dataset, DataLoader, Sampler與Collate_fn讀取資料\n",
    "這裡我們會需要兩個helper functions，其中一個是讀取資料與清洗資料的函式(load_review)，另外一個是生成詞向量函式\n",
    "(generate_vec)，注意這裡我們用來產生詞向量的方法是單純將文字tokenize(為了使產生的文本長度不同，而不使用BoW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_review(review_path):\n",
    "    with open(review_path, encoding='utf-8') as f:\n",
    "        review = f.read()\n",
    "\n",
    "    # 移除non-alphabet符號、贅字與tokenize\n",
    "    review = re.sub(r'\\W', ' ', review)\n",
    "    review = nltk.word_tokenize(review)\n",
    "    \n",
    "    return review\n",
    "\n",
    "\n",
    "def generate_vec(review, vocab_dic):\n",
    "    idx_vec = [vocab_dic[word] for word in review if vocab_dic.get(word)]\n",
    "\n",
    "    return idx_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#建立客製化dataset\n",
    "\n",
    "class dataset(Dataset):\n",
    "    '''custom dataset to load reviews and labels\n",
    "    Parameters\n",
    "    ----------\n",
    "    data_pairs: list\n",
    "        directory of all review-label pairs\n",
    "    vocab: list\n",
    "        list of vocabularies\n",
    "    '''\n",
    "    def __init__(self, data_dirs, vocab):\n",
    "        self.data_dirs = data_dirs\n",
    "        self.vocab = vocab\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_dirs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        review_path, label = self.data_dirs[idx]\n",
    "        review = load_review(review_path)\n",
    "        idx_vector = generate_vec(review, self.vocab)\n",
    "\n",
    "        return idx_vector, label\n",
    "    \n",
    "    \n",
    "\n",
    "#建立客製化collate_fn，將長度不一的文本pad 0 變成相同長度\n",
    "def collate_fn(batch):\n",
    "    reviews, labels = zip(*batch)\n",
    "    lengths = torch.LongTensor([len(review) for review in reviews])\n",
    "    labels = torch.LongTensor(labels)\n",
    "    reviews = pad_sequence([\n",
    "        torch.LongTensor(review) for review in reviews\n",
    "    ], batch_first=True, padding_value=0)\n",
    "\n",
    "    return reviews, labels, lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[  486,     4,   701,    25,     2,   884,    17, 75154, 75154,     6,\n",
       "            392,  4207,  3993, 32461, 18275,  3993,  7926, 33012,  9537,   292,\n",
       "           1953, 75154, 75154,    17,    74,    14,   268,  1783,    14,   466,\n",
       "           1288,  3220,   106,    92,   404,  3046,    17,    34,    10,   277,\n",
       "            599,  1266,   194,   101,  3993,  4207,    49,   787,   312,     8,\n",
       "             94,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0],\n",
       "         [ 4969,   101, 10457,  1069,   233,   295,   645,  1306,  1190, 40274,\n",
       "           1306,   536,    43,  2779,   386,   128,   637,  2661,    94,    34,\n",
       "            180,   476,     9,    80,  1411,   462,   373,  1679,   101,     1,\n",
       "            581,  1385,   482,   218,    33,    73,    17,     1,   208,   191,\n",
       "           4160,   666,     2,   307,    30,    43,    21,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0],\n",
       "         [   96,   217,    29,   123,    12,  1388,    64,   380,   206,    19,\n",
       "          28495,    19,  1124,     1,  2796,   178,    14,   212,  1124,    30,\n",
       "             48,    14,  1774,   217,     1,  1383,   888,  1192,   261,   354,\n",
       "            481,  1567,    15,     6,   659,    67,    33,  3391,  7892,   429,\n",
       "            651,     2,  2226,    18,   768,    55,    11,   371,   111,    18,\n",
       "             63,   199,  1649,  3391,  5239,   115,   244,     1,   115,   336,\n",
       "             22,   105,  1584,   247,    73,   323,  4021,   902,   583,   117,\n",
       "            921,   907,   204,     8,    29,   141,   152,    15,     4,    71,\n",
       "             25,    93,    29,    62,     2,   888,  5068,   141,    11,   293,\n",
       "            177,    37,   500,  1111,   651,   255,    21,    59,   752,    62,\n",
       "           1076,   329,  5387,   107,   549,   111,   473,  1118],\n",
       "         [  371,     1,     9,   255,  2625,     1,  1666, 75154, 75154,    11,\n",
       "             80,   733,  4101,   392,     1,    49,   148,    28,  7305,   270,\n",
       "             40,   135,  3930,   582,    49,    28,  7305,   147,   860,   212,\n",
       "          15656,  2783,   733,   281,    28,  5138,   333,   186,  1313,     1,\n",
       "           3520,   333,   139,  1823,  6038, 75154, 75154,   203,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0]]),\n",
       " tensor([0, 0, 0, 1]),\n",
       " tensor([ 51,  47, 108,  48]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使用Pytorch的RandomSampler來進行indice讀取並建立dataloader\n",
    "custom_dataset = dataset(review_pairs, vocab_dic)\n",
    "custom_dataloader = DataLoader(custom_dataset, \n",
    "                               batch_size=4, \n",
    "                               sampler=RandomSampler(custom_dataset), \n",
    "                               collate_fn=collate_fn\n",
    ")\n",
    "next(iter(custom_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
